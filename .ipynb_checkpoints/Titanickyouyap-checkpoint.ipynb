{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0.ライブラリのインポート"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pylab as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nfrom scipy.stats import mstats\nfrom tqdm import tqdm\n\nfrom sklearn import metrics, model_selection, feature_selection, ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\nfrom xgboost import XGBClassifier\nfrom imblearn import under_sampling, over_sampling\n\n\nfrom IPython.display import Image\nfrom io import StringIO\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.データの読み出し\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ngender_submission=pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\ndata = pd.concat([train, test], sort=True)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.特徴量設計"},{"metadata":{"trusted":true},"cell_type":"code","source":"#欠損値処理\ndata['Sex'].replace(['male','female'],[0, 1], inplace=True)\n\ndata['Embarked'].fillna(('S'), inplace=True)\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'].fillna(np.mean(data['Fare']), inplace=True)\n# data['Mr']=data.Name.apply(lambda x:1 if 'Mr' in x else 0)\n# data['Mrs']=data.Name.apply(lambda x:1 if 'Mrs' in x else 0)\n# data['Miss']=data.Name.apply(lambda x:1 if 'Miss' in x else 0)\nage_avg = data['Age'].mean()\n\ndata['Age'].fillna(int(age_avg), inplace=True)\n\ndelete_columns = [ 'PassengerId', ]\ndata.drop(delete_columns, axis = 1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Salutation'] = data.Name.str.extract(' ([A-Za-z]+).', expand=False)\ndata['Salutation'] = data['Salutation'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndata['Salutation'] = data['Salutation'].replace('Mlle', 'Miss')\ndata['Salutation'] = data['Salutation'].replace('Ms', 'Miss')\ndata['Salutation'] = data['Salutation'].replace('Mme', 'Mrs')\ndel data['Name']\nSalutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\ndata[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"] + 1\n\ndata['Salutation'] = data['Salutation'].map(Salutation_mapping)\ndata['Salutation'] = data['Salutation'].fillna(0)\n\n\ndata['Ticket_Lett'] = data['Ticket'].apply(lambda x: str(x)[0])\ndata['Ticket_Lett'] = data['Ticket_Lett'].apply(lambda x: str(x))\ndata['Ticket_Lett'] = np.where((data['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), data['Ticket_Lett'], np.where((data['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ndata['Ticket_Len'] = data['Ticket'].apply(lambda x: len(x))\ndel data['Ticket']\ndata['Ticket_Lett']=data['Ticket_Lett'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3)\n\n\ndata['Cabin_Lett'] = data['Cabin'].apply(lambda x: str(x)[0]) \ndata['Cabin_Lett'] = data['Cabin_Lett'].apply(lambda x: str(x)) \ndata['Cabin_Lett'] = np.where((data['Cabin_Lett']).isin([ 'F', 'E', 'D', 'C', 'B', 'A']),data['Cabin_Lett'], np.where((data['Cabin_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\ndel data['Cabin'] \ndata['Cabin_Lett']=data['Cabin_Lett'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1) \n\n# data['IsAlone'] = 0\n# data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[:len(train)]\ntest = data[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['Survived']\nX_train = train.drop('Survived', axis = 1)\nX_test = test.drop('Survived', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#生存者の割合\ntrain.Survived.sum()/train.Survived.count()\n#まぁ均衡ということでいいかも、一応　データの整形を行う\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn import under_sampling, over_sampling\ncols = train.columns.tolist()\ncols.remove('Survived')\n\npositive_cnt = int(train['Survived'].sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample  = rus.fit_sample(train[cols], train[['Survived']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_x_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#recvを用いて特徴量選択を行う\nfeature_importance_models = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    tree.DecisionTreeClassifier(),\n    XGBClassifier()\n]\n \nscoring = ['accuracy']\ndf_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\ndf_rfe_cols_cnt['cnt'] = 0\n \nfor i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n    \n    rfe = feature_selection.RFECV(model, step=3)\n    rfe.fit(data_x_sample, data_y_sample)\n    rfe_cols = train[cols].columns.values[rfe.get_support()]\n    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n    \ndf_rfe_cols_cnt.plot(kind='bar', figsize=(15, 5))\nplt.show()\n#まぁまぁどのパラメーターも効いている。家族総数のパラメータを追加してみる","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"familiynumber\"]=data.Parch+data.SibSp+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Familiynumberを設けてrecvを行う\n\ntrain = data[:len(train)]\ntest = data[len(train):]\ny_train = train['Survived']\nX_train = train.drop('Survived', axis = 1)\nX_test = test.drop('Survived', axis = 1)\n\ncols = train.columns.tolist()\ncols.remove('Survived')\n\npositive_cnt = int(train['Survived'].sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample  = rus.fit_sample(train[cols], train[['Survived']])\n#recvを用いて特徴量選択を行う\nfeature_importance_models = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    tree.DecisionTreeClassifier(),\n    XGBClassifier()\n]\n \nscoring = ['accuracy']\ndf_rfe_cols_cnt = pd.DataFrame(columns=['cnt'], index=cols)\ndf_rfe_cols_cnt['cnt'] = 0\n \nfor i, model in tqdm(enumerate(feature_importance_models), total=len(feature_importance_models)):\n    \n    rfe = feature_selection.RFECV(model, step=3)\n    rfe.fit(data_x_sample, data_y_sample)\n    rfe_cols = train[cols].columns.values[rfe.get_support()]\n    df_rfe_cols_cnt.loc[rfe_cols, 'cnt'] += 1\n    \ndf_rfe_cols_cnt.plot(kind='bar', figsize=(15, 5))\nplt.show()\n#家族の数を追加したところ、Parch,SibSpの重要度が減少した。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Embarked,Parch,SibSpを除いてモデル選択を行う場合はこちらのコメントアウトを外す。今回は多い特徴量で検討\n# x_cols = df_rfe_cols_cnt[df_rfe_cols_cnt['cnt'] >= 4].index\n# x_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Embarked,Parch,SibSpを除かずモデル選択を行う\nx_cols = df_rfe_cols_cnt.index\nx_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_cnt = int(train.Survived.sum())\nrus = under_sampling.RandomUnderSampler(sampling_strategy={0:positive_cnt, 1:positive_cnt}, random_state=0)\ndata_x_sample, data_y_sample = rus.fit_sample(train[x_cols], train[['Survived']])\n\nlen(data_x_sample), len(data_y_sample), data.Survived.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.モデル設計,評価"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 特徴量を選択して、複数のモデルで精度を調査する\nimport lightgbm as lgb\n \nmodels = [\n \n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n \n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.RidgeClassifierCV(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n \n    #xgboost\n    XGBClassifier(),\n    lgb.LGBMClassifier()    \n]\n \ndf_compare = pd.DataFrame(columns=['name', 'train_accuracy', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nfor model in tqdm(models):\n    \n    name = model.__class__.__name__\n    \n    cv_rlts = model_selection.cross_validate(model, data_x_sample, data_y_sample, scoring=scoring, cv=10, return_train_score=True)\n \n    for i in range(10):\n        s = pd.Series([name, cv_rlts['train_accuracy'][i], cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name=name+str(i))\n        df_compare = df_compare.append(s)\n        \nplt.figure(figsize=(12,8))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h', linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ぱっと見ABC,BC,ETSC,GBC,RFC,XGB,LRBC,LGBMあたりが良さそう"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 精度の良いモデルを選んで、投票モデルを学習\n \nvote_models = [\n \n    #Ensemble Methods\n    ('abc', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etsc', ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n \n    #Gaussian Processes\n    #('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM\n    ('lrcv', linear_model.LogisticRegressionCV()),\n    #('rccv', linear_model.RidgeClassifierCV()), # unable soft voting\n    \n    #Navies Bayes\n    #('bnb', naive_bayes.BernoulliNB()),\n    #('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor\n    #('knc', neighbors.KNeighborsClassifier()),\n    \n    #Trees    \n    #('dtc', tree.DecisionTreeClassifier()),\n    #('etc', tree.ExtraTreeClassifier()),\n    \n    #Discriminant Analysis\n    #('lda', discriminant_analysis.LinearDiscriminantAnalysis()),\n    #('qda', discriminant_analysis.QuadraticDiscriminantAnalysis()),\n \n    #xgboost\n    ('xgbc', XGBClassifier()),\n    \n    #lightgbm\n    ('lgbm',lgb.LGBMClassifier())\n    \n]\n \ndf_compare = pd.DataFrame(columns=['name', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nvote_hard_model = ensemble.VotingClassifier(estimators=vote_models, voting='hard')\ncv_rlts = model_selection.cross_validate(vote_hard_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['hard', cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='hard'+str(i))\n    df_compare = df_compare.append(s)\n    \nvote_soft_model = ensemble.VotingClassifier(estimators=vote_models , voting='soft')\ncv_rlts = model_selection.cross_validate(vote_soft_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['soft', cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='soft'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h', linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 各モデルのハイパーパラメータをグリッドサーチ\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\ngrid_param = [\n    \n    #AdaBoostClassifier\n    [{ \n        'n_estimators': grid_n_estimator, #default=50\n        'learning_rate': grid_learn, #default=1\n        #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n        'random_state': grid_seed\n    }],\n    \n    #BaggingClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'max_samples': grid_ratio, #default=1.0\n        'random_state': grid_seed\n     }],\n\n    #ExtraTreesClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=”gini”\n        'max_depth': grid_max_depth, #default=None\n        'random_state': grid_seed\n     }],\n\n    #GradientBoostingClassifier\n    [{\n        #'loss': ['deviance', 'exponential'], #default=’deviance’\n        'learning_rate': [.05], #default=0.1\n        'n_estimators': [300], #default=100\n        #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n        'max_depth': grid_max_depth, #default=3   \n        'random_state': grid_seed\n     }],\n\n    #RandomForestClassifier\n    [{\n        'n_estimators': grid_n_estimator, #default=10\n        'criterion': grid_criterion, #default=”gini”\n        'max_depth': grid_max_depth, #default=None\n        'oob_score': [True], #default=False\n        'random_state': grid_seed\n     }],\n    \n    #LogisticRegressionCV\n    [{\n        'fit_intercept': grid_bool, #default: True\n        #'penalty': ['l1','l2'],\n        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n        'random_state': grid_seed\n     }],\n    \n    # ExtraTreeClassifier\n    [{}],\n    \n    # LinearDiscriminantAnalysis\n    [{}],\n    \n    #XGBClassifier\n    [{\n        'learning_rate': grid_learn, #default: .3\n        'max_depth': [1,2,4,6,8,10], #default 2\n        'n_estimators': grid_n_estimator, \n        'seed': grid_seed  \n     }],\n    #LGBMClassifier\n    [{\n        'learning_rate':grid_learn,\n        'n_estimators':grid_n_estimator,\n        'max_depth':[1,2,4,6,8,10],\n        'min_child_weight':[0.5,1,2],\n        'min_child_samples':[5,10,20],\n        'subsample':[0.8],\n        'colsample_bytree':[0.8],\n        'verbose':[-1],\n        'num_leaves':[80]}]\n    \n]\n\nfor model, param in tqdm(zip(vote_models, grid_param), total=len(vote_models)):\n    \n    best_search = model_selection.GridSearchCV(estimator=model[1], param_grid=param, scoring='roc_auc')\n    best_search.fit(data_x_sample, data_y_sample)\n\n    best_param = best_search.best_params_\n    model[1].set_params(**best_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#　投票モデルの作成、softモデルとhardモデルの作成と検証\ndf_compare = pd.DataFrame(columns=['name', 'valid_accuracy', 'time'])\nscoring = ['accuracy']\n \nvote_hard_model = ensemble.VotingClassifier(estimators=vote_models, voting='hard')\ncv_rlts = model_selection.cross_validate(vote_hard_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['hard',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='hard'+str(i))\n    df_compare = df_compare.append(s)\n    \nvote_soft_model= ensemble.VotingClassifier(estimators=vote_models , voting='soft')\ncv_rlts = model_selection.cross_validate(vote_soft_model, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['soft',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='soft'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h',  linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_compare.groupby('name').mean().sort_values(by='valid_accuracy', ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"あんまりsoftもhardも変わらない。以下は均衡データを使って混合行列の作成\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\ntrain_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\nvote_soft_model.fit(train_x, train_y)\n\npred = vote_soft_model.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vote_hard_model.fit(train_x, train_y)\n\npred = vote_hard_model.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 動かすパラメータを明示的に表示\nparams = {\"learning_rate\":[0.1,0.3,0.5],\n        \"max_depth\": [2,3,5,10],\n         \"subsample\":[0.5,0.8,0.9,1],\n         \"colsample_bytree\": [0.5,1.0],\n         }\n# モデルにインスタンス生成\nmod = XGBClassifier()\n# ハイパーパラメータ探索\ngv =  model_selection.GridSearchCV(mod, params, cv = 10, scoring= 'roc_auc', n_jobs =-1)\n\n#　trainデータとtestデータに分割\ntrain_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\n\n# 予測モデルを作成\ngv.fit(train_x,train_y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gv =  model_selection.GridSearchCV(mod, params, cv = 10, scoring= 'roc_auc', n_jobs =-1)\ncv_rlts = model_selection.cross_validate(gv, data_x_sample, data_y_sample, cv=10, scoring=scoring)\nfor i in range(10):\n    s = pd.Series(['xgboost',  cv_rlts['test_accuracy'][i], cv_rlts['fit_time'][i]], index=df_compare.columns, name='xgb'+str(i))\n    df_compare = df_compare.append(s)\n    \nplt.figure(figsize=(12,3))\nsns.boxplot(data=df_compare, y='name', x='valid_accuracy', orient='h',  linewidth=0.5, width=0.5)\nplt.grid()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\ngv.fit(train_x, train_y)\n\npred = gv.predict(valid_x)\n\nfig, axs = plt.subplots(ncols=2,figsize=(15,5))\n\nsns.heatmap(metrics.confusion_matrix(valid_y, pred), vmin=0, annot=True, fmt='d', ax=axs[0])\naxs[0].set_xlabel('Predict')\naxs[0].set_ylabel('Ground Truth')\naxs[0].set_title('Accuracy: {}'.format(metrics.accuracy_score(valid_y, pred)))\nfpr, tpr, thresholds = metrics.roc_curve(valid_y, pred)\naxs[1].plot(fpr, tpr)\naxs[1].set_title('ROC curve')\naxs[1].set_xlabel('False Positive Rate')\naxs[1].set_ylabel('True Positive Rate')\naxs[1].grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"softの方がややいい感じがするので、softの投票モデルで提出\n"},{"metadata":{},"cell_type":"markdown","source":"# 4.モデル提出"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#最初はモデル評価のために均衡データの作成を行い、そのデータを分割しモデルの評価を行なった\n#提出するモデルは訓練データを全て使い学習する。データ数多い方が精度良くなりそうだし。\nfrom sklearn.model_selection import KFold, train_test_split\n# train_x, valid_x, train_y, valid_y = train_test_split(data_x_sample, data_y_sample, test_size=0.3, random_state=0)\n\nvote_soft_model.fit(X_train, y_train)\n\npred = vote_soft_model.predict(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame(pd.read_csv(\"/kaggle/input/titanic/test.csv\")['PassengerId'])\nsub['Survived'] = list(map(int, pred))\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}